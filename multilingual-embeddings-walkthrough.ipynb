{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc4e119b",
   "metadata": {},
   "source": [
    "# A walkthrough for multi-language document Q&A\n",
    "\n",
    "For my data journalism talk [How I convinced GPT to teach me about Hungarian folktales (without speaking a word of Hungarian)](https://github.com/jsoma/mediaparty-folktales) at Media Party Chicago 2023.\n",
    "\n",
    "There's a *lot* more you can do with this, this is just the very basics! Feel free to email me at [js4571@columbia.edu](mailto:js4571@columbia.edu) or on Twitter at [@dangerscarf](https://twitter.com/dangerscarf) if you want some more details.\n",
    "\n",
    "We're going to be using [langchain](https://python.langchain.com/en/latest/index.html) but you can absolutely use other great tools like [LlamaIndex](https://gpt-index.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507bdcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d933104",
   "metadata": {},
   "source": [
    "# Read in our text\n",
    "\n",
    "We're going to be asking questions about the book [Eredeti népmesék](https://www.google.com/books/edition/Eredeti_n%C3%A9pmes%C3%A9k/FcZSEAAAQBAJ?hl=en). We're getting the text version [from Project Gutenberg](https://www.gutenberg.org/files/38852/38852-0.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f3ab34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "# Gutenberg pretends everything is English, which\n",
    "# means \"Hát gyöngyömadta\" gets really mangled\n",
    "response = requests.get(\"https://www.gutenberg.org/files/38852/38852-0.txt\")\n",
    "text = response.content.decode(\"utf-8\")\n",
    "\n",
    "# Cleaning up newlines\n",
    "text = text.replace(\"\\r\", \"\")\n",
    "text = re.sub(\"\\n(?=[^\\n])\", \"\", text)\n",
    "\n",
    "# Saving the book\n",
    "with open('book.txt', 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b15d2",
   "metadata": {},
   "source": [
    "# Split up the text\n",
    "\n",
    "We'll divide our text up into 1,000-character chunks, with 100-character overlap between each set of neighboring chunks. We're loading text here, but you can load [all sorts of other kinds of documents](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) like HTML, PDFs and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f98ec2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('book.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f062e6",
   "metadata": {},
   "source": [
    "# Create embeddings for each passage\n",
    "\n",
    "There are lots lots lots of different multilingual embeddings available! We're using one called [`paraphrase-multilingual-MiniLM-L12-v2`](https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2). Different ones will perform better or worse, depending on many many *many* variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4f4dd5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name='paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "docsearch = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa5b7f",
   "metadata": {},
   "source": [
    "If you're curious, we can look at the embeddings for just _one_ passage. We can do it for **What did Zsuzska steal from the devil?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f3865e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = embeddings.embed_documents([\"What did Zsuzska steal from the devil?\"])[0]\n",
    "len(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f7a2a3",
   "metadata": {},
   "source": [
    "384 scores for the document! They aren't \"dog\" or \"wild\" or \"fuzzy\" or even mean anything specific, like in the talk, they're just... magic numbers that only mean things to the embedding model. We just have to trust that texts with similar scores are going to have similar content!\n",
    "\n",
    "If we wanted to look at the first twenty of those scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f3488b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.46812501549720764, 0.47161218523979187, -0.39475440979003906, 0.18969321250915527, 0.08756688982248306, 0.04914027079939842, 0.6678051948547363, 0.24234464764595032, 0.011556974612176418, 0.24045951664447784, 0.15715603530406952, 0.04403669759631157, 0.25661030411720276, -0.12375714629888535, -0.5067397356033325, 0.053942807018756866, 0.06712772697210312, 0.13114140927791595, -0.17556653916835785, 0.2375480830669403]\n"
     ]
    }
   ],
   "source": [
    "print(scores[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210eb4b6",
   "metadata": {},
   "source": [
    "## Find relevant passages using embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22d59f",
   "metadata": {},
   "source": [
    "To take that one step further, let's try to find one related passage. Here's a match for **What did Zsuzska steal from the devil?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "625fe62f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Hiába tagadta szegény Zsuzska, nem használt semmit, elindult hát nagyszomorúan. Épen éjfél volt, mikor az ördög házához ért, aludt az ördögis, a felesége is. Zsuzska csendesen belopódzott, ellopta a tenger-ütőpálczát, avval bekiáltott az ablakon.\\n– Hej ördög, viszem ám már a tenger-ütő pálczádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, most viszed a tenger-ütő pálczámat, de majdmeglakolsz te ezért.\\nUtána is szaladt, de megint csak a tengerparton tudott közel jutnihozzá, ott meg Zsuzska megütötte a tengert a tenger-ütő pálczával,kétfelé vált előtte, utána meg összecsapódott, megint nem foghatta megaz ördög. Zsuzska ment egyenesen a királyhoz.\\n– No felséges király, elhoztam már a tengerütő pálczát is.', metadata={'source': 'book.txt'})]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k=1 because we only want one result\n",
    "docsearch.similarity_search(\"What did Zsuzska steal from the devil?\", k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9460b4c4",
   "metadata": {},
   "source": [
    "I don't know Hungarian, but we can [translate the match with DeepL](https://www.deepl.com/en/translator).\n",
    "\n",
    "**Translation:**\n",
    "\n",
    "> Poor Zsuzska denied it in vain, it was of no use, so she set off in sorrow. It was only midnight when she reached the devil's house, and the devil's wife was asleep. Zuzska crept in quietly, stole the sea-whisk, and with it she called through the window.\n",
    "> \n",
    "> \"Hey, devil, I'll take your sea-rod too.\"\n",
    ">\n",
    "> \"Hey dog, Zsuzska, you killed three of my beautiful girls, you stole my sea-stepping-hip, now you're taking my sea-bat, but you'll get it for this.\"\n",
    "> \n",
    "> He ran after her, but again he could only get close to her on the beach, and there she hit the sea with her sea-beating stick, and it split in two in front of him, and then they clashed, and again the devil couldn't catch her. She went straight to the king.\n",
    "> \n",
    "> \"Well, sire King, I have brought the sea-striking stick.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3c783",
   "metadata": {},
   "source": [
    "Seems like a good match!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82899e0",
   "metadata": {},
   "source": [
    "## Filter, search, and send our question to GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c58a2eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our connection to GPT\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Your API key\n",
    "openai_api_key = \"sk-....\"\n",
    "\n",
    "# Use temperature=0 to get the same results every time\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0,\n",
    "    openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02da19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c68d0497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Zsuzska stole the devil's sea-striking stick, his golden cabbage head, and his golden baby in a golden cradle.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What did Zsuzska steal from the devil?\"\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31620a",
   "metadata": {},
   "source": [
    "# What about attribution?\n",
    "\n",
    "Do we want sources? We just need to add some sort of metadata to our passages. We'll do it in a very simple way right now: we're just **giving them numbers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cead3b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    }
   ],
   "source": [
    "# Loop through each document, adding an index\n",
    "for i in range(len(docs)):\n",
    "    docs[i].metadata['source'] = f\"passage-{i}\"\n",
    "\n",
    "docsearch = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1396f0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'passage-0', 'passage_index': 0}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now metadata has both a source and a passage_index\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ccaf3cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=docsearch.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d40d21c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What did Zsuzska steal from the devil?',\n",
       " 'verbose': True,\n",
       " 'answer': 'Zsuzska stole the tenger-ütő pálczát (a staff that can split the sea) from the devil.\\n',\n",
       " 'sources': 'passage-267, passage-268, passage-269, passage-266'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What did Zsuzska steal from the devil?\"\n",
    "\n",
    "chain({ \"question\": query, \"verbose\": True })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c02b27",
   "metadata": {},
   "source": [
    "Why is this answer different?? I don't know! Oh boy, looks like LLMs... *aren't perfect?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b715c7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Hiába tagadta szegény Zsuzska, nem használt semmit, elindult hát nagyszomorúan. Épen éjfél volt, mikor az ördög házához ért, aludt az ördögis, a felesége is. Zsuzska csendesen belopódzott, ellopta a tenger-ütőpálczát, avval bekiáltott az ablakon.\\n– Hej ördög, viszem ám már a tenger-ütő pálczádat is.\\n– Hej kutya Zsuzska, megöletted három szép lyányomat, elloptad atenger-lépő czipőmet, most viszed a tenger-ütő pálczámat, de majdmeglakolsz te ezért.\\nUtána is szaladt, de megint csak a tengerparton tudott közel jutnihozzá, ott meg Zsuzska megütötte a tengert a tenger-ütő pálczával,kétfelé vált előtte, utána meg összecsapódott, megint nem foghatta megaz ördög. Zsuzska ment egyenesen a királyhoz.\\n– No felséges király, elhoztam már a tengerütő pálczát is.', metadata={'source': 'passage-267', 'passage_index': 267})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then you can pull out the docs individually\n",
    "docs[267]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1b1be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
